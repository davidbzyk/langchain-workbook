{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    SentenceTransformersTokenTextSplitter,\n",
        "    TextSplitter,\n",
        "    TokenTextSplitter,\n",
        ")\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Define the directory containing the text file\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "file_path = os.path.join(current_dir, \"books\", \"romeo_and_juliet.txt\")\n",
        "db_dir = os.path.join(current_dir, \"db\")\n",
        "\n",
        "# Check if the text file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"The file {file_path} does not exist. Please check the path.\"\n",
        "    )\n",
        "\n",
        "# Read the text content from the file\n",
        "loader = TextLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")  # Update to a valid embedding model if needed\n",
        "\n",
        "\n",
        "# Function to create and persist vector store\n",
        "def create_vector_store(docs, store_name):\n",
        "    persistent_directory = os.path.join(db_dir, store_name)\n",
        "    if not os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Creating vector store {store_name} ---\")\n",
        "        db = Chroma.from_documents(\n",
        "            docs, embeddings, persist_directory=persistent_directory\n",
        "        )\n",
        "        print(f\"--- Finished creating vector store {store_name} ---\")\n",
        "    else:\n",
        "        print(\n",
        "            f\"Vector store {store_name} already exists. No need to initialize.\")\n",
        "\n",
        "\n",
        "# 1. Character-based Splitting\n",
        "# Splits text into chunks based on a specified number of characters.\n",
        "# Useful for consistent chunk sizes regardless of content structure.\n",
        "print(\"\\n--- Using Character-based Splitting ---\")\n",
        "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "char_docs = char_splitter.split_documents(documents)\n",
        "create_vector_store(char_docs, \"chroma_db_char\")\n",
        "\n",
        "# 2. Sentence-based Splitting\n",
        "# Splits text into chunks based on sentences, ensuring chunks end at sentence boundaries.\n",
        "# Ideal for maintaining semantic coherence within chunks.\n",
        "print(\"\\n--- Using Sentence-based Splitting ---\")\n",
        "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
        "sent_docs = sent_splitter.split_documents(documents)\n",
        "create_vector_store(sent_docs, \"chroma_db_sent\")\n",
        "\n",
        "# 3. Token-based Splitting\n",
        "# Splits text into chunks based on tokens (words or subwords), using tokenizers like GPT-2.\n",
        "# Useful for transformer models with strict token limits.\n",
        "print(\"\\n--- Using Token-based Splitting ---\")\n",
        "token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
        "token_docs = token_splitter.split_documents(documents)\n",
        "create_vector_store(token_docs, \"chroma_db_token\")\n",
        "\n",
        "# 4. Recursive Character-based Splitting\n",
        "# Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
        "# Balances between maintaining coherence and adhering to character limits.\n",
        "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
        "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100)\n",
        "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
        "create_vector_store(rec_char_docs, \"chroma_db_rec_char\")\n",
        "\n",
        "# 5. Custom Splitting\n",
        "# Allows creating custom splitting logic based on specific requirements.\n",
        "# Useful for documents with unique structure that standard splitters can't handle.\n",
        "print(\"\\n--- Using Custom Splitting ---\")\n",
        "\n",
        "\n",
        "class CustomTextSplitter(TextSplitter):\n",
        "    def split_text(self, text):\n",
        "        # Custom logic for splitting text\n",
        "        return text.split(\"\\n\\n\")  # Example: split by paragraphs\n",
        "\n",
        "\n",
        "custom_splitter = CustomTextSplitter()\n",
        "custom_docs = custom_splitter.split_documents(documents)\n",
        "create_vector_store(custom_docs, \"chroma_db_custom\")\n",
        "\n",
        "\n",
        "# Function to query a vector store\n",
        "def query_vector_store(store_name, query):\n",
        "    persistent_directory = os.path.join(db_dir, store_name)\n",
        "    if os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory, embedding_function=embeddings\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=\"similarity_score_threshold\",\n",
        "            search_kwargs={\"k\": 1, \"score_threshold\": 0.1},\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "\n",
        "# Define the user's question\n",
        "query = \"How did Juliet die?\"\n",
        "\n",
        "# Query each vector store\n",
        "query_vector_store(\"chroma_db_char\", query)\n",
        "query_vector_store(\"chroma_db_sent\", query)\n",
        "query_vector_store(\"chroma_db_token\", query)\n",
        "query_vector_store(\"chroma_db_rec_char\", query)\n",
        "query_vector_store(\"chroma_db_custom\", query)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}