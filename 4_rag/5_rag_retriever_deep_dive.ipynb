{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the persistent directory\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "db_dir = os.path.join(current_dir, \"db\")\n",
        "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "\n",
        "# Function to query a vector store with different search types and parameters\n",
        "def query_vector_store(\n",
        "    store_name, query, embedding_function, search_type, search_kwargs\n",
        "):\n",
        "    if os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory,\n",
        "            embedding_function=embedding_function,\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=search_type,\n",
        "            search_kwargs=search_kwargs,\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "\n",
        "# Define the user's question\n",
        "query = \"How did Juliet die?\"\n",
        "\n",
        "# Showcase different retrieval methods\n",
        "\n",
        "# 1. Similarity Search\n",
        "# This method retrieves documents based on vector similarity.\n",
        "# It finds the most similar documents to the query vector based on cosine similarity.\n",
        "# Use this when you want to retrieve the top k most similar documents.\n",
        "print(\"\\n--- Using Similarity Search ---\")\n",
        "query_vector_store(\"chroma_db_with_metadata\", query,\n",
        "                   embeddings, \"similarity\", {\"k\": 3})\n",
        "\n",
        "# 2. Max Marginal Relevance (MMR)\n",
        "# This method balances between selecting documents that are relevant to the query and diverse among themselves.\n",
        "# 'fetch_k' specifies the number of documents to initially fetch based on similarity.\n",
        "# 'lambda_mult' controls the diversity of the results: 1 for minimum diversity, 0 for maximum.\n",
        "# Use this when you want to avoid redundancy and retrieve diverse yet relevant documents.\n",
        "# Note: Relevance measures how closely documents match the query.\n",
        "# Note: Diversity ensures that the retrieved documents are not too similar to each other,\n",
        "#       providing a broader range of information.\n",
        "print(\"\\n--- Using Max Marginal Relevance (MMR) ---\")\n",
        "query_vector_store(\n",
        "    \"chroma_db_with_metadata\",\n",
        "    query,\n",
        "    embeddings,\n",
        "    \"mmr\",\n",
        "    {\"k\": 3, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
        ")\n",
        "\n",
        "# 3. Similarity Score Threshold\n",
        "# This method retrieves documents that exceed a certain similarity score threshold.\n",
        "# 'score_threshold' sets the minimum similarity score a document must have to be considered relevant.\n",
        "# Use this when you want to ensure that only highly relevant documents are retrieved, filtering out less relevant ones.\n",
        "print(\"\\n--- Using Similarity Score Threshold ---\")\n",
        "query_vector_store(\n",
        "    \"chroma_db_with_metadata\",\n",
        "    query,\n",
        "    embeddings,\n",
        "    \"similarity_score_threshold\",\n",
        "    {\"k\": 3, \"score_threshold\": 0.1},\n",
        ")\n",
        "\n",
        "print(\"Querying demonstrations with different search types completed.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}